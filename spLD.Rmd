---
title: "Sparse LD"
output:
  xaringan::moon_reader:
    seal: false
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE, width = 70)
knitr::opts_chunk$set(fig.align = 'center', dev = "svg", out.width = "80%",
                      echo = FALSE, comment = "", fig.width = 5, global.par = TRUE)
```

class: title-slide center middle inverse

<br>

# LD matrices for LDpred2

<br>

## sparsity and robustness

---

### LDpred2 and correlation matrices

<br>

LDpred2 (and lassosum2) only needs to compute $$\omega_j = {R_{.,j}}^T \beta,$$ where $R_{.,j}$ is the $j$-th column of the correlation (LD) matrix and $\beta$ is the current vector of (scaled) effect sizes sampled from the Gibbs sampler.

--

<br>

In practice, we keep track of the vector $\Omega$ (of all $\omega_j$), and update it using $$\Omega = \Omega + R_{.,j} \cdot (\beta_j^\text{new} - \beta_j^\text{old}).$$
Often $\beta_j^\text{new} = \beta_j^\text{old} = 0$, then there is no update needed.
Moreover, the update needs to happen only for indices such that $R_{.,j} \neq 0$.

--

<br>

**Basically, we need $R_{.,j}$.**

---

### Sparsity of LD matrices

<br>

- We consider that two variants further away than 3 cM are not correlated, which gives a sparse LD matrix with values around the diagonal only.

- We have introduced a *compact* sparse format where, for all $j$, we only store the first $i$ such that $R_{i,j} \neq 0$, and then store all non-zero $R_{i,j}$.

- We also use an optimal algorithm to detect independent blocks in the LD matrix; it adds a bit of sparsity (e.g. keeping 80% of non-zero values). It also improves robustness of LDpred2 (probably because small errors cannot accumulate/propagate outside of the blocks).

--

<br>

In practice, it leads to a ~15 GB data for 1,054,330 HapMap3 variants and ~30 GB data for 1,444,196 variants.

LDpred2 is currently fast enough (it generally takes less than one hour). The main issue if we want to use many more variants will probably be the memory required.

---

class: center middle inverse

## How to use even more variants?

---

### Use a more stringent LD splitting? (1/5)

By making smaller blocks, we can reduce the size by 50%. It also makes LDpred2 more robust, but also reduces its predictive performance when robustness is not an issue.

```{r, out.width = "85%"}
knitr::include_graphics("figures/hm3_plus_tight.png")
```

---

### Use (smaller) sliding blocks? (2/5)

```{r, out.width = "85%"}
knitr::include_graphics("figures/sliding_blocks.png")
```

It can reduces the size of the LD matrix by up to $7\times$. But leads to divergence issues for half of the phenotypes tested. This is expected given the smallest eigenvalue of the LD matrix becomes much smaller than 0.

---

### Use an eigen decomposition of the LD? (3/5)

<br>

This is used in SBayesRC, and they choose K (the number of eigenvectors kept) to reach 99.5% variance explained, which leads to a large K (they say 20% of M, where M is the size of a block).

Moreover, $R = W W^T$ in the corresponding block. Then it means we need to compute a matrix-vector product each time we want to access a column of $R$.

I cannot see how this could be a good solution for LDpred2.

---

### Store LD with 2 bytes (instead of 8)? (4/5)

<br>

Can store 65536 different values with 2 bytes, so why not values between -1 and 1 with good enough precision? 

- Can use [-1; 1] $\longleftrightarrow$ [-32767; 32767]

- This would reduce the size of the data by $4\times$

- The precision of the rounding is more than 4 decimal places

<br>

This rounding seems fine since it does not seem to change much the eigenvalues of the LD matrix.

---

### Use very sparse inverse of the LD matrix? (5/5)

<br> 

- Bjarni covered the LDGMs very recently in a JC

- For the largest block (11,497 variants in EUR), the precision matrix $P$ is 99.8% sparse  and takes only 1.8 Mb to store.

- It is estimated using a graphical lasso algorithm, which makes it positive definite (i.e. the smallest eigenvalue is positive).

- We can recover a column of the (rescaled) inverse (what we need) by solving the linear system $P R_{.,j} = I_{.,j}$.

- A fast way to solve this linear system is to use a Cholesky decomposition $P = L L^T$, but the triangular matrix $L$ is less sparse than $P$ and requires $24\times$ more memory.

- Using Eigen C++ library, we can make this decomposition in 1.2 sec and then solve 1000 linear systems in 6 more seconds. Therefore, it would require 2 hours to perform these computations in 1000 iterations (for one block only!).

---

class: center middle inverse

## How to make LDpred2 more robust?

<br>

### Use an LD matrix which is positive definite

---

### Use a regularized LD matrix

- Using the graphical lasso, which I have reimplemented (and tweaked to keep the sparsity of the input correlation matrix).    
When the regularization is large enough, the resulting correlation matrix is positive definite.

--

<br>

- Using the multiplicative shrinkage from Wen and Stephens (2010): $$\exp(\text{-}2 \cdot Ne / m \cdot c_{i,j}),$$
where $Ne$ and $m$ are population parameters, and $c_{i,j}$ is the distance in Morgan between variants $i$ and $j$. For $Ne = 11400$ and $m = 183$, we have $\exp(\text{-}1.25 \cdot d_{i,j})$ where $d_{i,j}$ is the distance in cM.    
In practice, it makes the smallest eigenvalue much closer to 0.

--

<br>

- Using a convex linear shrinkage $R_s = s R + (1 - s) I$.    
With $\lambda_\text{min}$ as the smallest eigenvalues of $R$, we can use $s = \frac{\epsilon - 1}{\lambda_\text{min} - 1}$ to get $\epsilon$ as the smallest eigenvalue of $R_s$.

---

### Some results

#### y-axis uses s=0.95 // x-axis uses the mult shrinkage + glasso

```{r, out.width = "85%"}
knitr::include_graphics("figures/hm3_plus_regul.png")
```

---

class: inverse, center, middle

# Thanks!

<br>

Presentation available at    
https://privefl.github.io/thesis-docs/spLD.html

<br>

<br>

`r icons::icon_style(fill = "white", icons::fontawesome$brands$twitter)` `r icons::icon_style(fill = "white", icons::fontawesome$brands$github)` privefl

.footnote[Slides created via the R package [**xaringan**](https://github.com/yihui/xaringan)]

